{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8d8b774-6665-41cb-88fd-65495e30a9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marat/miniconda/envs/whisper/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.utils.validation import check_symmetric\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac417b31-dd76-4919-9ed5-f7d4fcfb9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472d8ee-a9e0-49d0-a8e5-7a29a5f609fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similiarity_sents(sents, neigbors_count=20):\n",
    "    sent_ids = [i for i, s in enumerate(sents) if s['vector'] is not None]\n",
    "    V = cosine_similarity(np.array([sents[i]['vector'] for i in sent_ids]))\n",
    "\n",
    "    for i in range(len(V)):\n",
    "        if i-neigbors_count > 0:\n",
    "            V[i][:(i-neigbors_count)] = 0\n",
    "        V[i][(i+neigbors_count+1):] = 0\n",
    "\n",
    "    not_connected_sents = np.where(np.sum(V**2, axis=0) < 2)[0]\n",
    "    not_connected_sents\n",
    "\n",
    "    sent_ids = [si for i, si in enumerate(sent_ids) if i not in not_connected_sents]\n",
    "    V = np.delete(V, not_connected_sents, axis=0)\n",
    "    V = np.delete(V, not_connected_sents, axis=1)\n",
    "\n",
    "    check_symmetric(V, raise_exception=True)\n",
    "    assert len(V) == len(sent_ids)\n",
    "\n",
    "    return V, sent_ids\n",
    "\n",
    "\n",
    "def sent_ends(part):\n",
    "    return part.strip().endswith('.')\n",
    "\n",
    "\n",
    "def enrich_whisper(transcription):\n",
    "    segments = transcription['segments']\n",
    "    _s_ids = [s['id'] for s in segments]\n",
    "    assert len(set(_s_ids)) == len(_s_ids)\n",
    "\n",
    "    sents = []\n",
    "    parts = []\n",
    "    for s in segments:\n",
    "        parts.append(segments[s['id']])\n",
    "        if sent_ends(parts[-1]['text']):\n",
    "            text = ''.join(p['text'] for p in parts).strip()\n",
    "            vs = [tok.vector for tok in nlp(text) if not tok.is_stop and tok.is_alpha]\n",
    "            if vs:\n",
    "                v = np.sum(vs, axis=0)\n",
    "                v_norm = v / (np.linalg.norm(v) + 1e-10)\n",
    "            else:\n",
    "                v_norm = None\n",
    "            sents.append({\n",
    "                'segment_id': parts[0]['id'],\n",
    "                'start': parts[0]['start'],\n",
    "                'text': text,\n",
    "                'vector': v_norm\n",
    "            })\n",
    "            parts = []\n",
    "    return sents\n",
    "\n",
    "\n",
    "def suppress_lonely_labels(L):\n",
    "    R = np.array(L)\n",
    "    changes_count = len(R)\n",
    "    iter_count = 0\n",
    "    while changes_count > 0 and iter_count < 10:\n",
    "        R_next = np.array(R)\n",
    "        for i in range(1, len(R)-1):\n",
    "            if R[i-1] == R[i+1] and R[i] != R[i-1]:\n",
    "                R_next[i] = R[i-1]\n",
    "        changes_count = (R != R_next).sum()\n",
    "        R = R_next\n",
    "        iter_count += 1\n",
    "    return R\n",
    "\n",
    "\n",
    "assert list(suppress_lonely_labels([0, 1, 0])) == [0, 0, 0]\n",
    "assert list(suppress_lonely_labels([0, 1, 0, 1, 0])) == [0, 0, 0, 0, 0]\n",
    "assert list(suppress_lonely_labels([0, 1, 2, 1, 0])) == [0, 1, 1, 1, 0]\n",
    "suppress_lonely_labels([\n",
    "        4,  4,  4,  4,  4,  4,  4, 17, 17, 17, 17, 17,  4, 17,  4,  4, 17,\n",
    "       17,  4,  4,  4, 17, 17, 17, 17, 17, 17,  6,  6,  6,  6,  6,  6,  6,\n",
    "        6,  6,  6,  6,  6,  6,  6,  6,  6, 12, 12, 12, 12, 12, 12,  0, 12,\n",
    "       12,  0,  0, 12,  0, 12, 12,  0, 12, 12, 12, 12, 12,  0,  0,  0,  0,\n",
    "       12,  0,  0,  9,  0,  9,  9,  9,  0,  9,  9,  9,  9,  9,  9,  9,  9,\n",
    "        9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 14, 14, 14,  1,  1,  1,  1,\n",
    "        1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,  1,  1,\n",
    "        1, 11, 11, 11, 15, 11, 11, 11, 11, 11, 11, 15, 11, 11, 11, 15, 11,\n",
    "       11, 11, 11, 11, 15, 19, 19, 19, 19, 15, 19, 19, 19, 19, 19, 19,  5,\n",
    "       15, 13,  5,  5, 13,  5, 13, 13, 13, 13, 13,  5, 13, 13,  5, 13,  5,\n",
    "        5,  5, 13,  5, 13, 13,  5, 13,  7, 13, 13,  8,  8,  8,  8,  8,  8,\n",
    "        8,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  2,\n",
    "        2,  7,  2,  2,  2,  2,  2, 16, 16, 16,  2, 16,  2,  2,  2,  2,  2,\n",
    "        2,  2, 16, 16, 16, 10, 16, 10, 10, 10, 10, 10, 10, 10, 10,  3, 10,\n",
    "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  3,  3,  3, 18,  3,  3,\n",
    "        3, 18, 18, 18, 18, 18, 18])\n",
    "\n",
    "def merge_overlapping_labels(L):\n",
    "    boundery = {}\n",
    "    for i, l in enumerate(L):\n",
    "        if l not in boundery:\n",
    "            boundery[l] = [i, i]\n",
    "        else:\n",
    "            boundery[l][1] = i\n",
    "\n",
    "    clusters = []\n",
    "    i = 0\n",
    "    while i < len(L):\n",
    "        current_cluster = {'labels': {L[i]}, 'boundery': boundery[L[i]]}\n",
    "        while i < current_cluster['boundery'][1]:\n",
    "            if L[i] not in current_cluster['labels']:\n",
    "                current_cluster['labels'].add(L[i])\n",
    "                current_cluster['boundery'][1] = max(current_cluster['boundery'][1], boundery[L[i]][1])\n",
    "            i += 1\n",
    "        clusters.append(current_cluster)\n",
    "        i += 1\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "assert merge_overlapping_labels(np.array([\n",
    "        4,  4,  4,  4,  4,  4,  4, 17, 17, 17, 17, 17,  4, 17,  4,  4, 17,\n",
    "       17,  4,  4,  4, 17, 17, 17, 17, 17, 17,  6,  6,  6,  6,  6,  6,  6,\n",
    "        6,  6,  6,  6,  6,  6,  6,  6,  6, 12, 12, 12, 12, 12, 12,  0, 12,\n",
    "       12,  0,  0, 12,  0, 12, 12,  0, 12, 12, 12, 12, 12,  0,  0,  0,  0,\n",
    "       12,  0,  0,  9,  0,  9,  9,  9,  0,  9,  9,  9,  9,  9,  9,  9,  9,\n",
    "        9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 14, 14, 14,  1,  1,  1,  1,\n",
    "        1, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,  1,  1,\n",
    "        1, 11, 11, 11, 15, 11, 11, 11, 11, 11, 11, 15, 11, 11, 11, 15, 11,\n",
    "       11, 11, 11, 11, 15, 19, 19, 19, 19, 15, 19, 19, 19, 19, 19, 19,  5,\n",
    "       15, 13,  5,  5, 13,  5, 13, 13, 13, 13, 13,  5, 13, 13,  5, 13,  5,\n",
    "        5,  5, 13,  5, 13, 13,  5, 13,  7, 13, 13,  8,  8,  8,  8,  8,  8,\n",
    "        8,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  2,\n",
    "        2,  7,  2,  2,  2,  2,  2, 16, 16, 16,  2, 16,  2,  2,  2,  2,  2,\n",
    "        2,  2, 16, 16, 16, 10, 16, 10, 10, 10, 10, 10, 10, 10, 10,  3, 10,\n",
    "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  3,  3,  3, 18,  3,  3,\n",
    "        3, 18, 18, 18, 18, 18, 18])) == \\\n",
    "    [\n",
    "        {'labels': {4, 17}, 'boundery': [0, 26]},\n",
    "        {'labels': {6}, 'boundery': [27, 42]},\n",
    "        {'labels': {0, 9, 12}, 'boundery': [43, 94]},\n",
    "        {'labels': {1, 14}, 'boundery': [95, 119]},\n",
    "        {'labels': {2, 3, 5, 7, 8, 10, 11, 13, 15, 16, 18, 19}, 'boundery': [120, 261]}\n",
    "    ]\n",
    "\n",
    "\n",
    "def split_into_chapters(sents, sents_are_words=False):\n",
    "    V, sent_ids = similiarity_sents(sents, neigbors_count=100 if sents_are_words else 20)\n",
    "    clusters_count = int(len(sent_ids)/10/10) if sents_are_words else int(len(sent_ids)/10)\n",
    "    if clusters_count < 2:\n",
    "        clusters = [{'boundery': [0, len(sent_ids)-2], 'start': 0.0}]\n",
    "    else:\n",
    "        sc = SpectralClustering(affinity=\"precomputed\", n_clusters=clusters_count, assign_labels=\"discretize\").fit(np.abs(V))\n",
    "        labels = suppress_lonely_labels(sc.labels_)\n",
    "        clusters = merge_overlapping_labels(labels)\n",
    "\n",
    "    for k, c in enumerate(clusters):\n",
    "        # t = nice_time(sents[sent_ids[c['boundery'][0]]]['start'])\n",
    "        start_id = sent_ids[c['boundery'][0]]\n",
    "        try:\n",
    "            end_id = sent_ids[c['boundery'][1]+1]\n",
    "        except IndexError:\n",
    "            end_id = len(sents)\n",
    "        texts = [sents[i]['text'] for i in range(start_id, end_id)]\n",
    "        text = (' ' if sents_are_words else '').join(texts)\n",
    "        yield {'text': text,\n",
    "               'start': sents[sent_ids[c['boundery'][0]]]['start']}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:whisper]",
   "language": "python",
   "name": "conda-env-whisper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
