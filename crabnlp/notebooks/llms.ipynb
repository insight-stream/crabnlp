{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd9279f7-353e-4914-aa76-e94a65424905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "from time import monotonic\n",
    "import asyncio\n",
    "from typing import Optional, List, Callable\n",
    "\n",
    "import tiktoken\n",
    "import openai\n",
    "from openai.error import RateLimitError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf43d21-d822-4711-8be0-46132823e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "GPT_MAX_CONTEXT_LEN = 4096\n",
    "tok = tiktoken.encoding_for_model(GPT_MODEL_NAME)\n",
    "openai.api_key = os.environ['OPENAI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b64183-ab45-4686-9b17-9fa499e6759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tlen(text: str) -> int:\n",
    "    return len(tok.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83e5bf3-0891-4864-8cf7-b3198634a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_overhead(messages):\n",
    "    return int(tlen(repr(messages)) * 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2ecafd-24f0-4bb1-9d68-02f8d29055fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_a_text(text: str, max_tokens: int, overlapping: Optional[int | float] = None):\n",
    "    if overlapping is None:\n",
    "        overlapping = int(max_tokens/10)\n",
    "    elif isinstance(overlapping, float):\n",
    "        overlapping = int(max_tokens * overlapping)\n",
    "    t = tok.encode(text)\n",
    "    for i in range(0, len(t), max_tokens-overlapping):\n",
    "        yield tok.decode(t[i:(i+max_tokens)]).strip()\n",
    "\n",
    "\n",
    "assert list(chunk_a_text(\"1 2 3 4 5 6 7 8 9 10\", 8, 4)) == \\\n",
    "        ['1 2 3 4', '3 4 5 6', '5 6 7 8', '7 8 9 10', '9 10']\n",
    "\n",
    "\n",
    "def chunk_texts(lines: List[str], max_tokens=GPT_MAX_CONTEXT_LEN-256, join_lines_char=''):\n",
    "    \"\"\"Chunk a list of lines, try minimazing spliting a line \n",
    "    (for instance, every line is a sentence)\"\"\"\n",
    "    if not lines:\n",
    "        return\n",
    "    for i in range(len(lines), 1, -1):\n",
    "        if tlen(join_lines_char.join(lines[:i])) <= max_tokens:\n",
    "            yield join_lines_char.join(lines[:i])\n",
    "            yield from chunk_texts(lines[max(i, 0):], max_tokens=max_tokens, join_lines_char=join_lines_char)\n",
    "            break\n",
    "    else:\n",
    "        yield from chunk_a_text(lines[0], max_tokens=max_tokens)\n",
    "        yield from chunk_texts(lines[1:], max_tokens=max_tokens, join_lines_char=join_lines_char)\n",
    "\n",
    "\n",
    "assert list(chunk_texts(['hello world!', 'one two three'], 2)) == ['hello world', '!', 'one two', 'three']\n",
    "assert list(chunk_texts(['one two three', 'four five six', 'seven eight nine ten'], 6, join_lines_char=' ')) \\\n",
    " == ['one two three four five six', 'seven eight nine ten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e266aa-d11d-4758-b234-d7eb9e96c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff(*exceptions, max_wait_sec=60, start=1, base=1.5):\n",
    "    if not exceptions:\n",
    "        exceptions = [Exception]\n",
    "\n",
    "    def wrapper(func):\n",
    "        @functools.wraps(func)\n",
    "        async def wrapped(*args, **kwargs):\n",
    "            start_time = monotonic()\n",
    "            i = 0\n",
    "            while monotonic() - start_time < max_wait_sec + 0.1:\n",
    "                try:\n",
    "                    return await func(*args, **kwargs)\n",
    "                except Exception as ex:\n",
    "                    if not isinstance(ex, tuple(exceptions)):\n",
    "                        raise ex\n",
    "                    print(f'Backing off `{func.__name__}`')\n",
    "                    wait_time = min(start*base**i, start_time + max_wait_sec - monotonic())\n",
    "                    if wait_time < 0:\n",
    "                        raise ex\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                    i += 1\n",
    "        return wrapped\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56483873-7114-4837-8832-970bd997f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff(RateLimitError)\n",
    "async def acreate_chatcompletion(messages, model_name=GPT_MODEL_NAME):\n",
    "    return await openai.ChatCompletion.acreate(\n",
    "      model=model_name,\n",
    "      messages=messages\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5fa7613-b94d-41d9-89da-0eda7c73bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def map_chatgpt(messages_generator: Callable[[str], List], text: str | tuple | list,\n",
    "                      answer_ratio=1/3, overlapping=0.1) -> tuple[List[str], int]:\n",
    "    overhead = calc_overhead(messages_generator(''))\n",
    "\n",
    "    chunk_size = int((GPT_MAX_CONTEXT_LEN-overhead)*(1-answer_ratio))\n",
    "    if isinstance(text, str):\n",
    "        chunking = chunk_a_text(text, chunk_size, overlapping=overlapping)\n",
    "    else:\n",
    "        chunking = chunk_texts(text, chunk_size)\n",
    "\n",
    "    tasks = []\n",
    "    for c in chunking:\n",
    "        gi_messages = messages_generator(c)\n",
    "        tasks.append(asyncio.create_task(acreate_chatcompletion(gi_messages)))\n",
    "    resps = await asyncio.gather(*tasks)\n",
    "    results, toks = zip(*[(r['choices'][0]['message']['content'], r['usage']['total_tokens']) for r in resps])\n",
    "    return results, sum(toks)\n",
    "\n",
    "\n",
    "async def map_chatgpt_recursive(messages_generator: Callable[[str], List], text: str | tuple | list,\n",
    "                                answer_ratio=1/3, min_improvement=0.3,\n",
    "                                tokens_already_used=0) -> tuple[List[str], int]:\n",
    "    tok_before = tlen(text if isinstance(text, str) else ' '.join(text))\n",
    "    results, tok_used = await map_chatgpt(messages_generator, text)\n",
    "    tok_after = tlen(' '.join(results))\n",
    "    if tok_after > (1-min_improvement)*tok_before:\n",
    "        return results, tokens_already_used + tok_used\n",
    "    else:\n",
    "        return await map_chatgpt_recursive(messages_generator, results, answer_ratio=answer_ratio,\n",
    "                                           min_improvement=min_improvement, \n",
    "                                           tokens_already_used=tokens_already_used + tok_used)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:crabnlp]",
   "language": "python",
   "name": "conda-env-crabnlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
