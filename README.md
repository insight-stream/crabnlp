<h1 align="center">Crabnlp Speaker Diarization Using OpenAI Whisper</h1>
<p align="center">
  <a href="https://github.com/insight-stream/crabnlp/stargazers">
    <img src="https://img.shields.io/github/stars/insight-stream/crabnlp.svg?colorA=orange&colorB=orange&logo=github"
         alt="GitHub stars">
  </a>
  <a href="https://github.com/insight-stream/crabnlp/issues">
        <img src="https://img.shields.io/github/issues/insight-stream/crabnlp.svg"
             alt="GitHub issues">
  </a>
  <a href="https://github.com/insight-stream/crabnlp/blob/master/LICENSE">
        <img src="https://img.shields.io/github/license/insight-stream/crabnlp.svg"
             alt="GitHub license">
  </a>
  </a>
  <a href="https://colab.research.google.com/github/insight-stream/crabnlp/blob/main/Whisper_Transcription_%2B_NeMo_Diarization.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab">
  </a>
 
</p>


# 
Crabnlp Speaker Diarization pipeline based on OpenAI Whisper

<img src="https://github.blog/wp-content/uploads/2020/09/github-stars-logo_Color.png" alt="drawing" width="25"/> **Please, star the project on github (see top-right corner) if you appreciate my contribution to the community!**

## What is it
This repository combines Whisper ASR capabilities with Voice Activity Detection (VAD) and Speaker Embedding to identify the speaker for each sentence in the transcription generated by Whisper. First, the vocals are extracted from the audio to increase the speaker embedding accuracy, then the transcription is generated using Whisper, then the timestamps are corrected and aligned using WhisperX to help minimize diarization error due to time shift. The audio is then passed into MarbleNet for VAD and segmentation to exclude silences, TitaNet is then used to extract speaker embeddings to identify the speaker for each segment, the result is then associated with the timestamps generated by WhisperX to detect the speaker for each word based on timestamps and then realigned using punctuation models to compensate for minor time shifts.


## Installation
Install the requirements
```
cd crabnlp
pip install -e crabnlp
```

### Example using transcribe
```
from crabnlp.transcribe import transcribe

transcription_result = transcribe('path/to/file')
```


## Known Limitations
- Overlapping speakers are yet to be addressed, a possible approach would be to separate the audio file and isolate only one speaker, then feed it into the pipeline but this will need much more computation
- There might be some errors, please raise an issue if you encounter any.

## Future Improvements
- Implement a maximum length per sentence for SRT

## Acknowledgements
This work is based on [OpenAI's Whisper](https://github.com/openai/whisper) , [Faster Whisper](https://github.com/guillaumekln/faster-whisper) , [Nvidia NeMo](https://github.com/NVIDIA/NeMo) , and [Facebook's Demucs](https://github.com/facebookresearch/demucs)